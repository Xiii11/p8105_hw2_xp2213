---
title: "P8105 Homework 2"
output: github_document
---
Name: Xi Peng   UNI: xp2213    Date: 10.02.2024

```{r setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(haven)
library(tidyr)
library(dplyr)
```


# Problem 1 NYC Transit Data

## Section 1: Data Import

```{r, message=FALSE}

NYCtransit_df =
    read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".", " ")) |>
  janitor::clean_names() |> 
  unite("routes_served", c(route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11), sep = ",", remove = TRUE, na.rm = TRUE) |> 
  select(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada) |>  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  ) |> 
  relocate(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada)

```

The NYC Transit Subway Entrance and Exit dataset contains information about different subway station's entrances and exits. Other detailed information, such as which subway lines stop at which station and their geographic coordinates, ADA compliance, and detailed information about the entrances, staffing status, etc.In this step, I cleaned the dataset by only keep the required variables: `r names(NYCtransit_df)`. The variable "entry" also converted to `r class(pull(NYCtransit_df,entry))`. After the cleaning step, the tidy dataset contains `r nrow(NYCtransit_df)` rows and `r ncol(NYCtransit_df)` columns.


## Section 2
```{r}

distinct_station =
  NYCtransit_df |> distinct(station_name,line)

ADA_compliant_station =
  NYCtransit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name,line)

Entry_without_vending = 
  NYCtransit_df |> 
  filter(vending == "NO", entry == "TRUE") |> 
  distinct(station_name,line)

numEntry_without_vending = nrow(Entry_without_vending)
numdistinct_station = nrow(distinct_station)

prop_without_vending = numEntry_without_vending/numdistinct_station

```

In this dataset, there are `r nrow(distinct_station)` distinct stations and `r nrow(ADA_compliant_station)` stations are ADA compliant. The proportion of station entrances / exits without vending allow entrance is `r prop_without_vending`.


## Section 3
```{r}

station_served_A = NYCtransit_df |> 
  filter(grepl("A", routes_served)) |> 
  distinct(station_name, line) |> 
  nrow()
  
ADA_compliant_A = NYCtransit_df |> 
  filter(grepl("A", routes_served), ada == TRUE) |> 
  distinct(station_name,line) |> 
  nrow()

```

There are `r station_served_A` distinct stations serve the A train and `r ADA_compliant_A` of the stations that serve the A train are ADA compliant.



# Problem 2 Trash Wheel Dataset

## Section 1: Data Import & Cleaning of the "Mr. Trash Wheel" Sheet

```{r}

Mr_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", 
  col_types = c("numeric","text","text","date","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","skip","skip"), skip = 1) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )

```

In section 1, the original dataset was imported, and the "Mr. Trash Wheel" sheet was selected. After examining the content of the original excel dataset, I identified columns labeled "x15" and "x16" as non-data columns containing only NAs. Therefore, these two columns were removed. Additionally, any rows that did not correspond to a specific dumpster were filtered out from the dataset. Lastly, the values in the "sports_balls" column were rounded to nearest integer and converted to integer variable.


## Section 2: Cleaning of the "Professor Trash Wheel" Sheet and "Gwynnda Trash Wheel" Sheet

```{r}

Prof_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster) 

Gwynnda_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster)

```

In section 2, similar steps were taken to clean the "Professor Trash Wheel" sheet and "Gwynnda Trash Wheel" sheet. After importing the original datasets of these two sheets, their contents were examined to ensure that no non-data entries were present. Also, any rows that did not correspond to a specific dumpster were filtered out from the datasets.

In all three datasets, columns that are not considered non-data but contain many NA values are kept. Similarly, rows that lack useful information are also retained as long as they include dumpster-specific data.


## Section 3: Combination of "Mr. Trash Wheel", "Professor Trash Wheel", and "Gwynnda Trash Wheel" sheets

```{r}

Mr_wheel_com = Mr_wheel |> mutate(trash_wheel_device = "Mr.", year = as.numeric(year))
Prof_wheel_com = Prof_wheel |> mutate(trash_wheel_device = "Professor")
Gwynnda_wheel_com = Gwynnda_wheel |> mutate(trash_wheel_device = "Gwynnda")

trash_wheel_df = 
  bind_rows(Mr_wheel_com, Prof_wheel_com, Gwynnda_wheel_com) |> 
  janitor::clean_names() |> 
  relocate(trash_wheel_device)

total_weight_trash_Prof = trash_wheel_df |> 
  filter(trash_wheel_device == "Professor") |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()

total_cigarette_Gwynnda = trash_wheel_df |> 
  filter(trash_wheel_device == "Gwynnda", month == "June", year == 2022 ) |> 
  drop_na(cigarette_butts) |> 
  pull(cigarette_butts) |> 
  sum()

```

In section 3, the three sheets were combined. The combined dataset contains a total of `r nrow(trash_wheel_df)` observations and `r ncol(trash_wheel_df)` columns. The variables included in this dataset are: `r names(trash_wheel_df)`. An additional column,"trash_wheel_device", was created based on the Trash Wheel Website introduction to denote which specific trash wheel device each observation belongs to. Among these varaibles, both the "dumpster" variable, which represents the dumpster number, and the "trash_wheel_device" variable are essential indicators provide timely and effective access to critical and detailed information for each observation. Therefore, I relocated the "trash_wheel_device" to a forward position, specifically to the first column in the dataset, to facilitate easier information extraction.Based on provided data, the total weight of trash collected by Professor Trash Wheel was `r total_weight_trash_Prof` tons, and the total number of cigarette butts collected by Gwynnda in June of 2022 was `r total_cigarette_Gwynnda`.



# Problem 3 Great British Bake Off Dataset

## Section 1: Data import and Creation of a single, well-organized dataset

```{r, message=FALSE}

bakers_df =
  read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "last_name"), sep = "")

bakes_df =
  read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  mutate(
    series = as.numeric(series)
  )

results_df =
  read_csv("data/gbb_datasets/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  mutate(
    result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated", 
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )

```

### check for completeness and correctness across datasets.

```{r}
anti_join(bakers_df,bakes_df)

anti_join(bakers_df,results_df)

anti_join(results_df,bakes_df)
```

### combination

```{r}
GBbakeoff_df = results_df |> 
  left_join(bakes_df, by = c("series","baker","episode")) |> 
  left_join(bakers_df, by = c("series","baker"))


```


In this section, the datasets were imported, and I reviewed the contents of each dataset. I noticed that the bakers' names in the baker.csv file were full names, whereas the other datasets only contained the bakers' first names. To ensure consistency across the datasets, I split the full names in baker.csv into separate columns for first and last names. After verifying the consistency between the first_name and last_name columns and the original baker_name colomn, I removed the baker_name variable to avoid redundant information. I then labeled the first_name as baker in this dataset, consistent with other datasets.Additionally, in the results.csv dataset, the first two rows contains non-data values, so they have been removed.
