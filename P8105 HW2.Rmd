---
title: "P8105 Homework 2"
output: github_document
---
Name: Xi Peng   UNI: xp2213    Date: 10.02.2024

```{r setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(haven)
library(tidyr)
library(dplyr)
```


# Problem 1 NYC Transit Data

## Section 1: Data import and cleaning

```{r, message=FALSE}

NYCtransit_df =
    read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".", " ")) |>
  janitor::clean_names() |> 
  unite("routes_served", c(route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11), sep = ",", remove = TRUE, na.rm = TRUE) |> 
  select(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada) |>  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  ) |> 
  relocate(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada)

```

The NYC Transit Subway Entrance and Exit dataset contains information about different subway station's entrances and exits. Other detailed information, such as which subway lines stop at which station and their geographic coordinates, ADA compliance, and detailed information about the entrances, staffing status, etc.In this step, I cleaned the dataset by only keep the required variables: `r names(NYCtransit_df)`. The variable "entry" also converted to `r class(pull(NYCtransit_df,entry))`. After the cleaning step, the tidy dataset contains `r nrow(NYCtransit_df)` rows and `r ncol(NYCtransit_df)` columns.


## Section 2: Distinct stations and ADA compliant Status
```{r}

distinct_station =
  NYCtransit_df |> distinct(station_name,line)

ADA_compliant_station =
  NYCtransit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name,line)

Entry_without_vending = 
  NYCtransit_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()

```

In this dataset, there are `r nrow(distinct_station)` distinct stations and `r nrow(ADA_compliant_station)` stations are ADA compliant. The proportion of station entrances / exits without vending allow entrance is `r Entry_without_vending`.


## Section 3: Distinct station serving the A train and ADA compliance status 
```{r}

station_served_A = NYCtransit_df |> 
  filter(grepl("A", routes_served)) |> 
  distinct(station_name, line) |> 
  nrow()
  
ADA_compliant_A = NYCtransit_df |> 
  filter(grepl("A", routes_served), ada == TRUE) |> 
  distinct(station_name,line) |> 
  nrow()

```

There are `r station_served_A` distinct stations serve the A train and `r ADA_compliant_A` of the stations that serve the A train are ADA compliant.



# Problem 2 Trash Wheel Dataset

## Section 1: Data import & cleaning of the "Mr. Trash Wheel" Sheet

```{r}

Mr_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", 
  col_types = c("numeric","text","text","date","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","skip","skip"), skip = 1) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )

```

In section 1, the original dataset was imported, and the "Mr. Trash Wheel" sheet was selected. After examining the content of the original excel dataset, I identified columns labeled "x15" and "x16" as non-data columns containing only NAs. Therefore, these two columns were removed. The first row, which contained figure but not actual data, was removed. Additionally, any rows that did not correspond to a specific dumpster were filtered out from the dataset. Lastly, the values in the "sports_balls" column were rounded to nearest integer and converted to integer variable.


## Section 2: Cleaning of the "Professor Trash Wheel" sheet and "Gwynnda Trash Wheel" sheet

```{r}

Prof_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster) 

Gwynnda_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster)

```

In section 2, similar steps were taken to clean the "Professor Trash Wheel" sheet and "Gwynnda Trash Wheel" sheet. After importing the original datasets of these two sheets, their contents were examined to ensure that no non-data entries were present. Also, any rows that did not correspond to a specific dumpster were filtered out from the datasets.

In all three datasets, columns that are not considered non-data but contain many NA values are kept. Similarly, rows that lack useful information are also retained as long as they include dumpster-specific data.


## Section 3: Combination of "Mr. Trash Wheel", "Professor Trash Wheel", and "Gwynnda Trash Wheel" sheets

```{r}

Mr_wheel_com = Mr_wheel |> mutate(trash_wheel_device = "Mr.", year = as.numeric(year))
Prof_wheel_com = Prof_wheel |> mutate(trash_wheel_device = "Professor")
Gwynnda_wheel_com = Gwynnda_wheel |> mutate(trash_wheel_device = "Gwynnda")

trash_wheel_df = 
  bind_rows(Mr_wheel_com, Prof_wheel_com, Gwynnda_wheel_com) |> 
  janitor::clean_names() |> 
  relocate(trash_wheel_device)

total_weight_trash_Prof = trash_wheel_df |> 
  filter(trash_wheel_device == "Professor") |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()

total_cigarette_Gwynnda = trash_wheel_df |> 
  filter(trash_wheel_device == "Gwynnda", month == "June", year == 2022 ) |> 
  drop_na(cigarette_butts) |> 
  pull(cigarette_butts) |> 
  sum()

```

In section 3, the three sheets were combined. The combined dataset contains a total of `r nrow(trash_wheel_df)` observations and `r ncol(trash_wheel_df)` columns. The variables included in this dataset are: `r names(trash_wheel_df)`. An additional column,"trash_wheel_device", was created based on the Trash Wheel Website introduction to denote which specific trash wheel device each observation belongs to. Among these varaibles, both the "dumpster" variable, which represents the dumpster number, and the "trash_wheel_device" variable are essential indicators provide timely and effective access to critical and detailed information for each observation. Therefore, I relocated the "trash_wheel_device" to a forward position, specifically to the first column in the dataset, to facilitate easier information extraction.Based on provided data, the total weight of trash collected by Professor Trash Wheel was `r total_weight_trash_Prof` tons, and the total number of cigarette butts collected by Gwynnda in June of 2022 was `r total_cigarette_Gwynnda`.



# Problem 3 Great British Bake Off Dataset

## Section 1: Data import and cleaning of bakers.csv, bakes.csv, and results.csv

```{r, message=FALSE}

bakers_df =
  read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  mutate(
    baker_first_name = word(baker_name, 1),
    baker_last_name = word(baker_name, -1)
  ) |> 
  select(-baker_name)

bakes_df =
  read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)

results_df =
  read_csv("data/gbb_datasets/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(
    result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated", 
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )

```

In this section, the datasets were imported, and I reviewed the contents of each dataset. I noticed that the bakers' names in the baker.csv file were full names, whereas the other datasets only contained the bakers' first names. To ensure consistency across the datasets, I split the full names in baker.csv into separate columns as baker_first_name and baker_last_names. After verifying the consistency between the first_name and last_name columns and the original baker_name colomn, I removed the baker_name variable to avoid redundant information. I then labeled the baker as baker_first_name in all datasets, ensuring consistency. Additionally, in the results.csv dataset, the first two rows contains non-data values, so they have been removed.The values of result column in the results.csv were converted into more descriptive phrases, which is useful for readability and interpretation.

## Section 2: Check for completeness and correctness across datasets.

```{r}
anti_join(bakers_df,bakes_df)

anti_join(bakers_df,results_df)

anti_join(results_df,bakes_df)
```

## Section 3: Combining and organizing data into a meaningful structure

```{r}

GBbakeoff_df = results_df |> 
  left_join(bakes_df, by = c("series","baker_first_name","episode")) |> 
  left_join(bakers_df, by = c("baker_first_name","series")) |> 
  relocate(series, episode, baker_first_name, baker_last_name, baker_age, baker_occupation, hometown, technical, result,signature_bake, show_stopper)
  
```

In this combined dataset, there are `r nrow(GBbakeoff_df)` rows and `r ncol(GBbakeoff_df)` columns.

Exporting the final dataset as a CSV:

```{r}

write_csv(GBbakeoff_df,"data/GBbakeoff_df.csv")

```

## Section 4: Table creation for Star baker/Winner of each episode in Seasons 5 through 10

```{r}

Star_win_df = 
  GBbakeoff_df |> 
  filter(series >= 5 & series <= 10, result == "Star Baker"| result == "Series Winner") |> 
  select(series, episode, baker_first_name, baker_last_name, baker_age, result, baker_occupation, hometown)

knitr::kable(
  Star_win_df, caption = "Table of Star baker/Winner of each episode in Seasons 5 through 10 "
  )

```

In this section, the question asks about the star baker/winner of each episode in Seasons 5 through 10. I filtered out these persons and formed a reader-friendly table. The key information, such as the which series and episode the star baker/winner attend and results for illustrating they were either star baker or winner, were placed in front. Other personally relevant information of each participant was placed afterwards. 

According to the table, the first thing I noticed is that Richard Burr earned "Star Baker" title in five episodes of Series 5. However, despite his strong performance, Nancy Birtwhistle was the "Series Winner" for that series, and she also won "Star Baker" once. In Series 6, Nadiya Hussain won the "Star Baker" title for four time and also earned the "Series Winner" title. Similarly, in Series 7, Candice Brown won the most "Star baker" titles and ultimately become the "Series Winner". In Series 8, although Steven Carter-Bailey earned the most "Star Baker" titles, Sophie Faldo was the "Series Winner". In Series 9, Rahul Mandal earned the most "Star Baker" titles and won the "Series Winner". In Series 10, Steph Blackwell earned the most "Star Baker" titles but David Atherton was the "Series Winner".

In conclusion, based on the patterns observed in this table, it is clear that there were no any predictable overall winners. The correlation between winning multiple "Star Baker" titles and winning the "Series Winner" is not consistent. What stands out the most is that multiple_time "Star Bakers" do not always end up with winning the series, which goes against what many people might expect. This makes the outcome of each series more surprising and unpredictable.

## Section 5: Dataset exploration of viewers.csv

```{r,message=FALSE}

viewers_df =
  read_csv("data/gbb_datasets/viewers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names()

```

The first 10 rows of this dataset:

```{r}

head(viewers_df,10)

```

```{r}

Ave_viewership1 = mean(pull(viewers_df,series_1), na.rm = TRUE)
  
Ave_viewership5 = mean(pull(viewers_df,series_5))

```

By checking the original imported dataset, I found there are missing values (NAs) for series_1 whereas series_5 contains no missing value. 

The average viewership for Season 1 was `r Ave_viewership1`, and for Season 5 was `r Ave_viewership5`.

