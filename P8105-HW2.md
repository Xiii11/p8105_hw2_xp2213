P8105 Homework 2
================

Name: Xi Peng UNI: xp2213 Date: 10.02.2024

# Problem 1 NYC Transit Data

## Section 1: Data Import

``` r
NYCtransit_df =
    read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".", " ")) |>
  janitor::clean_names() |> 
  unite("routes_served", c(route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11), sep = ",", remove = TRUE, na.rm = TRUE) |> 
  select(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada) |>  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  ) |> 
  relocate(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada)
```

The NYC Transit Subway Entrance and Exit dataset contains information
about different subway station’s entrances and exits. Other detailed
information, such as which subway lines stop at which station and their
geographic coordinates, ADA compliance, and detailed information about
the entrances, staffing status, etc.In this step, I cleaned the dataset
by only keep the required variables: line, station_name,
station_latitude, station_longitude, routes_served, entry, vending,
entrance_type, ada. The variable “entry” also converted to logical.
After the cleaning step, the tidy dataset contains 1868 rows and 9
columns.

## Section 2

``` r
distinct_station =
  NYCtransit_df |> distinct(station_name,line)

ADA_compliant_station =
  NYCtransit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name,line)

Entry_without_vending = 
  NYCtransit_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

In this dataset, there are 465 distinct stations and 84 stations are ADA
compliant. The proportion of station entrances / exits without vending
allow entrance is 0.3770492.

## Section 3

``` r
station_served_A = NYCtransit_df |> 
  filter(grepl("A", routes_served)) |> 
  distinct(station_name, line) |> 
  nrow()
  
ADA_compliant_A = NYCtransit_df |> 
  filter(grepl("A", routes_served), ada == TRUE) |> 
  distinct(station_name,line) |> 
  nrow()
```

There are 60 distinct stations serve the A train and 17 of the stations
that serve the A train are ADA compliant.

# Problem 2 Trash Wheel Dataset

## Section 1: Data Import & Cleaning of the “Mr. Trash Wheel” Sheet

``` r
Mr_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", 
  col_types = c("numeric","text","text","date","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","skip","skip"), skip = 1) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )
```

In section 1, the original dataset was imported, and the “Mr. Trash
Wheel” sheet was selected. After examining the content of the original
excel dataset, I identified columns labeled “x15” and “x16” as non-data
columns containing only NAs. Therefore, these two columns were removed.
Additionally, any rows that did not correspond to a specific dumpster
were filtered out from the dataset. Lastly, the values in the
“sports_balls” column were rounded to nearest integer and converted to
integer variable.

## Section 2: Cleaning of the “Professor Trash Wheel” Sheet and “Gwynnda Trash Wheel” Sheet

``` r
Prof_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster) 

Gwynnda_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster)
```

In section 2, similar steps were taken to clean the “Professor Trash
Wheel” sheet and “Gwynnda Trash Wheel” sheet. After importing the
original datasets of these two sheets, their contents were examined to
ensure that no non-data entries were present. Also, any rows that did
not correspond to a specific dumpster were filtered out from the
datasets.

In all three datasets, columns that are not considered non-data but
contain many NA values are kept. Similarly, rows that lack useful
information are also retained as long as they include dumpster-specific
data.

## Section 3: Combination of “Mr. Trash Wheel”, “Professor Trash Wheel”, and “Gwynnda Trash Wheel” sheets

``` r
Mr_wheel_com = Mr_wheel |> mutate(trash_wheel_device = "Mr.", year = as.numeric(year))
Prof_wheel_com = Prof_wheel |> mutate(trash_wheel_device = "Professor")
Gwynnda_wheel_com = Gwynnda_wheel |> mutate(trash_wheel_device = "Gwynnda")

trash_wheel_df = 
  bind_rows(Mr_wheel_com, Prof_wheel_com, Gwynnda_wheel_com) |> 
  janitor::clean_names() |> 
  relocate(trash_wheel_device)

total_weight_trash_Prof = trash_wheel_df |> 
  filter(trash_wheel_device == "Professor") |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()

total_cigarette_Gwynnda = trash_wheel_df |> 
  filter(trash_wheel_device == "Gwynnda", month == "June", year == 2022 ) |> 
  drop_na(cigarette_butts) |> 
  pull(cigarette_butts) |> 
  sum()
```

In section 3, the three sheets were combined. The combined dataset
contains a total of 1033 observations and 15 columns. The variables
included in this dataset are: trash_wheel_device, dumpster, month, year,
date, weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
homes_powered. An additional column,“trash_wheel_device”, was created
based on the Trash Wheel Website introduction to denote which specific
trash wheel device each observation belongs to. Among these varaibles,
both the “dumpster” variable, which represents the dumpster number, and
the “trash_wheel_device” variable are essential indicators provide
timely and effective access to critical and detailed information for
each observation. Therefore, I relocated the “trash_wheel_device” to a
forward position, specifically to the first column in the dataset, to
facilitate easier information extraction.Based on provided data, the
total weight of trash collected by Professor Trash Wheel was 246.74
tons, and the total number of cigarette butts collected by Gwynnda in
June of 2022 was 1.812^{4}.

# Problem 3 Great British Bake Off Dataset

## Section 1: Data import and Creation of a single, well-organized dataset

``` r
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "last_name"), sep = "")
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 120 rows [1, 2, 3, 4, 5, 6,
    ## 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].

``` r
bakes_df =
  read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)

results_df =
  read_csv("data/gbb_datasets/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(
    result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated", 
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### Check for completeness and correctness across datasets.

``` r
anti_join(bakers_df,bakes_df)
```

    ## Joining with `by = join_by(baker_first_name, series)`

    ## # A tibble: 120 × 6
    ##    baker_first_name last_name series baker_age baker_occupation         hometown
    ##    <chr>            <chr>      <dbl>     <dbl> <chr>                    <chr>   
    ##  1 ""               A              4        25 Charity worker           Saltley…
    ##  2 ""               A             10        28 Geography teacher        Essex   
    ##  3 ""               A              6        37 Nurse                    Brackne…
    ##  4 ""               A             10        24 Fashion designer         Halifax 
    ##  5 ""               A              7        25 Aerospace engineer       Derby /…
    ##  6 ""               A              1        30 Midwife                  Essex   
    ##  7 ""               A              9        30 Banker                   London  
    ##  8 ""               B              4        31 Military Wives' Choir S… Aldersh…
    ##  9 ""               B              2        31 Graphic Designer         Northam…
    ## 10 ""               B              7        23 Teaching assistant       South L…
    ## # ℹ 110 more rows

``` r
anti_join(bakers_df,results_df)
```

    ## Joining with `by = join_by(baker_first_name, series)`

    ## # A tibble: 120 × 6
    ##    baker_first_name last_name series baker_age baker_occupation         hometown
    ##    <chr>            <chr>      <dbl>     <dbl> <chr>                    <chr>   
    ##  1 ""               A              4        25 Charity worker           Saltley…
    ##  2 ""               A             10        28 Geography teacher        Essex   
    ##  3 ""               A              6        37 Nurse                    Brackne…
    ##  4 ""               A             10        24 Fashion designer         Halifax 
    ##  5 ""               A              7        25 Aerospace engineer       Derby /…
    ##  6 ""               A              1        30 Midwife                  Essex   
    ##  7 ""               A              9        30 Banker                   London  
    ##  8 ""               B              4        31 Military Wives' Choir S… Aldersh…
    ##  9 ""               B              2        31 Graphic Designer         Northam…
    ## 10 ""               B              7        23 Teaching assistant       South L…
    ## # ℹ 110 more rows

``` r
anti_join(results_df,bakes_df)
```

    ## Joining with `by = join_by(series, episode, baker_first_name)`

    ## # A tibble: 596 × 5
    ##    series episode baker_first_name technical result
    ##     <dbl>   <dbl> <chr>                <dbl> <chr> 
    ##  1      1       2 Lea                     NA <NA>  
    ##  2      1       2 Mark                    NA <NA>  
    ##  3      1       3 Annetha                 NA <NA>  
    ##  4      1       3 Lea                     NA <NA>  
    ##  5      1       3 Louise                  NA <NA>  
    ##  6      1       3 Mark                    NA <NA>  
    ##  7      1       4 Annetha                 NA <NA>  
    ##  8      1       4 Jonathan                NA <NA>  
    ##  9      1       4 Lea                     NA <NA>  
    ## 10      1       4 Louise                  NA <NA>  
    ## # ℹ 586 more rows

### Combination

``` r
GBbakeoff_df = results_df |> 
  left_join(bakes_df, by = c("series","baker_first_name","episode")) |> 
  left_join(bakers_df, by = c("baker_first_name","series")) |> 
  relocate(series, episode, baker_first_name,last_name)
```

In this section, the datasets were imported, and I reviewed the contents
of each dataset. I noticed that the bakers’ names in the baker.csv file
were full names, whereas the other datasets only contained the bakers’
first names. To ensure consistency across the datasets, I split the full
names in baker.csv into separate columns for first and last names. After
verifying the consistency between the first_name and last_name columns
and the original baker_name colomn, I removed the baker_name variable to
avoid redundant information. I then labeled the first_name as baker in
this dataset, consistent with other datasets.Additionally, in the
results.csv dataset, the first two rows contains non-data values, so
they have been removed.
