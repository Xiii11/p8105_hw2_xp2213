P8105 Homework 2
================

Name: Xi Peng UNI: xp2213 Date: 10.02.2024

# Problem 1 NYC Transit Data

## Section 1: Data Import

``` r
NYCtransit_df =
    read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".", " ")) |>
  janitor::clean_names() |> 
  unite("routes_served", c(route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11), sep = ",", remove = TRUE, na.rm = TRUE) |> 
  select(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada) |>  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  ) |> 
  relocate(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada)
```

The NYC Transit Subway Entrance and Exit dataset contains information
about different subway station’s entrances and exits. Other detailed
information, such as which subway lines stop at which station and their
geographic coordinates, ADA compliance, and detailed information about
the entrances, staffing status, etc.In this step, I cleaned the dataset
by only keep the required variables: line, station_name,
station_latitude, station_longitude, routes_served, entry, vending,
entrance_type, ada. The variable “entry” also converted to logical.
After the cleaning step, the tidy dataset contains 1868 rows and 9
columns.

## Section 2

``` r
distinct_station =
  NYCtransit_df |> distinct(station_name,line)

ADA_compliant_station =
  NYCtransit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name,line)

Entry_without_vending = 
  NYCtransit_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

In this dataset, there are 465 distinct stations and 84 stations are ADA
compliant. The proportion of station entrances / exits without vending
allow entrance is 0.3770492.

## Section 3

``` r
station_served_A = NYCtransit_df |> 
  filter(grepl("A", routes_served)) |> 
  distinct(station_name, line) |> 
  nrow()
  
ADA_compliant_A = NYCtransit_df |> 
  filter(grepl("A", routes_served), ada == TRUE) |> 
  distinct(station_name,line) |> 
  nrow()
```

There are 60 distinct stations serve the A train and 17 of the stations
that serve the A train are ADA compliant.

# Problem 2 Trash Wheel Dataset

## Section 1: Data Import & Cleaning of the “Mr. Trash Wheel” Sheet

``` r
Mr_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", 
  col_types = c("numeric","text","text","date","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","skip","skip"), skip = 1) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )
```

In section 1, the original dataset was imported, and the “Mr. Trash
Wheel” sheet was selected. After examining the content of the original
excel dataset, I identified columns labeled “x15” and “x16” as non-data
columns containing only NAs. Therefore, these two columns were removed.
The first row, which contained figure but not actual data, was removed.
Additionally, any rows that did not correspond to a specific dumpster
were filtered out from the dataset. Lastly, the values in the
“sports_balls” column were rounded to nearest integer and converted to
integer variable.

## Section 2: Cleaning of the “Professor Trash Wheel” Sheet and “Gwynnda Trash Wheel” Sheet

``` r
Prof_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster) 

Gwynnda_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster)
```

In section 2, similar steps were taken to clean the “Professor Trash
Wheel” sheet and “Gwynnda Trash Wheel” sheet. After importing the
original datasets of these two sheets, their contents were examined to
ensure that no non-data entries were present. Also, any rows that did
not correspond to a specific dumpster were filtered out from the
datasets.

In all three datasets, columns that are not considered non-data but
contain many NA values are kept. Similarly, rows that lack useful
information are also retained as long as they include dumpster-specific
data.

## Section 3: Combination of “Mr. Trash Wheel”, “Professor Trash Wheel”, and “Gwynnda Trash Wheel” sheets

``` r
Mr_wheel_com = Mr_wheel |> mutate(trash_wheel_device = "Mr.", year = as.numeric(year))
Prof_wheel_com = Prof_wheel |> mutate(trash_wheel_device = "Professor")
Gwynnda_wheel_com = Gwynnda_wheel |> mutate(trash_wheel_device = "Gwynnda")

trash_wheel_df = 
  bind_rows(Mr_wheel_com, Prof_wheel_com, Gwynnda_wheel_com) |> 
  janitor::clean_names() |> 
  relocate(trash_wheel_device)

total_weight_trash_Prof = trash_wheel_df |> 
  filter(trash_wheel_device == "Professor") |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()

total_cigarette_Gwynnda = trash_wheel_df |> 
  filter(trash_wheel_device == "Gwynnda", month == "June", year == 2022 ) |> 
  drop_na(cigarette_butts) |> 
  pull(cigarette_butts) |> 
  sum()
```

In section 3, the three sheets were combined. The combined dataset
contains a total of 1033 observations and 15 columns. The variables
included in this dataset are: trash_wheel_device, dumpster, month, year,
date, weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
homes_powered. An additional column,“trash_wheel_device”, was created
based on the Trash Wheel Website introduction to denote which specific
trash wheel device each observation belongs to. Among these varaibles,
both the “dumpster” variable, which represents the dumpster number, and
the “trash_wheel_device” variable are essential indicators provide
timely and effective access to critical and detailed information for
each observation. Therefore, I relocated the “trash_wheel_device” to a
forward position, specifically to the first column in the dataset, to
facilitate easier information extraction.Based on provided data, the
total weight of trash collected by Professor Trash Wheel was 246.74
tons, and the total number of cigarette butts collected by Gwynnda in
June of 2022 was 1.812^{4}.

# Problem 3 Great British Bake Off Dataset

## Section 1: Data import and Creation of a single, well-organized dataset

``` r
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  mutate(
    baker_first_name = word(baker_name, 1),
    baker_last_name = word(baker_name, -1)
  ) |> 
  select(-baker_name)

bakes_df =
  read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)

results_df =
  read_csv("data/gbb_datasets/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(
    result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated", 
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

In this section, the datasets were imported, and I reviewed the contents
of each dataset. I noticed that the bakers’ names in the baker.csv file
were full names, whereas the other datasets only contained the bakers’
first names. To ensure consistency across the datasets, I split the full
names in baker.csv into separate columns as baker_first_name and
baker_last_names. After verifying the consistency between the first_name
and last_name columns and the original baker_name colomn, I removed the
baker_name variable to avoid redundant information. I then labeled the
baker as baker_first_name in all datasets, ensuring consistency.
Additionally, in the results.csv dataset, the first two rows contains
non-data values, so they have been removed.The values of result column
in the results.csv were converted into more descriptive phrases, which
is useful for readability and interpretation.

## Section 2: Check for completeness and correctness across datasets.

``` r
anti_join(bakers_df,bakes_df)
```

    ## Joining with `by = join_by(series, baker_first_name)`

    ## # A tibble: 26 × 6
    ##    series baker_age baker_occupation   hometown baker_first_name baker_last_name
    ##     <dbl>     <dbl> <chr>              <chr>    <chr>            <chr>          
    ##  1     10        28 Geography teacher  Essex    Alice            Fevronia       
    ##  2     10        24 Fashion designer   Halifax  Amelia           LeBruin        
    ##  3      9        30 Banker             London   Antony           Amourdoux      
    ##  4      9        33 Full-time parent   Bristol  Briony           Williams       
    ##  5      9        36 Full-time parent   London   Dan              Beasley-Harling
    ##  6     10        32 Support worker     Rotherh… Dan              Chambers       
    ##  7     10        36 International hea… Whitby   David            Atherton       
    ##  8     10        40 Online project ma… Leeds    Helena           Garcia         
    ##  9     10        20 Student            Durham   Henry            Bird           
    ## 10      9        33 Countryside recre… County … Imelda           McCarron       
    ## # ℹ 16 more rows

``` r
anti_join(bakers_df,results_df)
```

    ## Joining with `by = join_by(series, baker_first_name)`

    ## # A tibble: 1 × 6
    ##   series baker_age baker_occupation hometown    baker_first_name baker_last_name
    ##    <dbl>     <dbl> <chr>            <chr>       <chr>            <chr>          
    ## 1      2        41 Housewife        Ongar, Ess… Jo               Wheatley

``` r
anti_join(results_df,bakes_df)
```

    ## Joining with `by = join_by(series, episode, baker_first_name)`

    ## # A tibble: 596 × 5
    ##    series episode baker_first_name technical result
    ##     <dbl>   <dbl> <chr>                <dbl> <chr> 
    ##  1      1       2 Lea                     NA <NA>  
    ##  2      1       2 Mark                    NA <NA>  
    ##  3      1       3 Annetha                 NA <NA>  
    ##  4      1       3 Lea                     NA <NA>  
    ##  5      1       3 Louise                  NA <NA>  
    ##  6      1       3 Mark                    NA <NA>  
    ##  7      1       4 Annetha                 NA <NA>  
    ##  8      1       4 Jonathan                NA <NA>  
    ##  9      1       4 Lea                     NA <NA>  
    ## 10      1       4 Louise                  NA <NA>  
    ## # ℹ 586 more rows

## Section 3: Combining and organizing data into a meaningful structure

``` r
GBbakeoff_df = results_df |> 
  left_join(bakes_df, by = c("series","baker_first_name","episode")) |> 
  left_join(bakers_df, by = c("baker_first_name","series")) |> 
  relocate(series, episode, baker_first_name, baker_last_name, baker_age, baker_occupation, hometown, technical, result,signature_bake, show_stopper)
```

Exporting the final dataset as a CSV:

``` r
write_csv(GBbakeoff_df,"data/GBbakeoff_df.csv")
```

## Section 4:
