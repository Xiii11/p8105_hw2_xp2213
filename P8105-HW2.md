P8105 Homework 2
================

Name: Xi Peng UNI: xp2213 Date: 10.02.2024

# Problem 1 NYC Transit Data

## Section 1: Data import and cleaning

``` r
NYCtransit_df =
    read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".", " ")) |>
  janitor::clean_names() |> 
  unite("routes_served", c(route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11), sep = ",", remove = TRUE, na.rm = TRUE) |> 
  select(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada) |>  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  ) |> 
  relocate(line, station_name, station_latitude, station_longitude, routes_served, entry, vending, entrance_type, ada)
```

The NYC Transit Subway Entrance and Exit dataset contains information
about different subway station’s entrances and exits. Other detailed
information, such as which subway lines stop at which station and their
geographic coordinates, ADA compliance, and detailed information about
the entrances, staffing status, etc.In this step, I cleaned the dataset
by only keep the required variables: line, station_name,
station_latitude, station_longitude, routes_served, entry, vending,
entrance_type, ada. The variable “entry” also converted to logical.
After the cleaning step, the tidy dataset contains 1868 rows and 9
columns.

## Section 2: Distinct stations and ADA compliant Status

``` r
distinct_station =
  NYCtransit_df |> distinct(station_name,line)

ADA_compliant_station =
  NYCtransit_df |> 
  filter(ada == TRUE) |> 
  distinct(station_name,line)

Entry_without_vending = 
  NYCtransit_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

In this dataset, there are 465 distinct stations and 84 stations are ADA
compliant. The proportion of station entrances / exits without vending
allow entrance is 0.3770492.

## Section 3: Distinct station serving the A train and ADA compliance status

``` r
station_served_A = NYCtransit_df |> 
  filter(grepl("A", routes_served)) |> 
  distinct(station_name, line) |> 
  nrow()
  
ADA_compliant_A = NYCtransit_df |> 
  filter(grepl("A", routes_served), ada == TRUE) |> 
  distinct(station_name,line) |> 
  nrow()
```

There are 60 distinct stations serve the A train and 17 of the stations
that serve the A train are ADA compliant.

# Problem 2 Trash Wheel Dataset

## Section 1: Data import & cleaning of the “Mr. Trash Wheel” Sheet

``` r
Mr_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", 
  col_types = c("numeric","text","text","date","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","skip","skip"), skip = 1) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )
```

In section 1, the original dataset was imported, and the “Mr. Trash
Wheel” sheet was selected. After examining the content of the original
excel dataset, I identified columns labeled “x15” and “x16” as non-data
columns containing only NAs. Therefore, these two columns were removed.
The first row, which contained figure but not actual data, was removed.
Additionally, any rows that did not correspond to a specific dumpster
were filtered out from the dataset. Lastly, the values in the
“sports_balls” column were rounded to nearest integer and converted to
integer variable.

## Section 2: Cleaning of the “Professor Trash Wheel” sheet and “Gwynnda Trash Wheel” sheet

``` r
Prof_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster) 

Gwynnda_wheel = 
  read_excel("Data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") |> 
  janitor::clean_names() |> 
  drop_na(dumpster)
```

In section 2, similar steps were taken to clean the “Professor Trash
Wheel” sheet and “Gwynnda Trash Wheel” sheet. After importing the
original datasets of these two sheets, their contents were examined to
ensure that no non-data entries were present. Also, any rows that did
not correspond to a specific dumpster were filtered out from the
datasets.

In all three datasets, columns that are not considered non-data but
contain many NA values are kept. Similarly, rows that lack useful
information are also retained as long as they include dumpster-specific
data.

## Section 3: Combination of “Mr. Trash Wheel”, “Professor Trash Wheel”, and “Gwynnda Trash Wheel” sheets

``` r
Mr_wheel_com = Mr_wheel |> mutate(trash_wheel_device = "Mr.", year = as.numeric(year))
Prof_wheel_com = Prof_wheel |> mutate(trash_wheel_device = "Professor")
Gwynnda_wheel_com = Gwynnda_wheel |> mutate(trash_wheel_device = "Gwynnda")

trash_wheel_df = 
  bind_rows(Mr_wheel_com, Prof_wheel_com, Gwynnda_wheel_com) |> 
  janitor::clean_names() |> 
  relocate(trash_wheel_device)

total_weight_trash_Prof = trash_wheel_df |> 
  filter(trash_wheel_device == "Professor") |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()

total_cigarette_Gwynnda = trash_wheel_df |> 
  filter(trash_wheel_device == "Gwynnda", month == "June", year == 2022 ) |> 
  drop_na(cigarette_butts) |> 
  pull(cigarette_butts) |> 
  sum()
```

In section 3, the three sheets were combined. The combined dataset
contains a total of 1033 observations and 15 columns. The variables
included in this dataset are: trash_wheel_device, dumpster, month, year,
date, weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
homes_powered. An additional column,“trash_wheel_device”, was created
based on the Trash Wheel Website introduction to denote which specific
trash wheel device each observation belongs to. Among these varaibles,
both the “dumpster” variable, which represents the dumpster number, and
the “trash_wheel_device” variable are essential indicators provide
timely and effective access to critical and detailed information for
each observation. Therefore, I relocated the “trash_wheel_device” to a
forward position, specifically to the first column in the dataset, to
facilitate easier information extraction.Based on provided data, the
total weight of trash collected by Professor Trash Wheel was 246.74
tons, and the total number of cigarette butts collected by Gwynnda in
June of 2022 was 1.812^{4}.

# Problem 3 Great British Bake Off Dataset

## Section 1: Data import of bakers.csv, bakes.csv, and results.csv, and creation of a single, well-organized dataset

``` r
bakers_df =
  read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  mutate(
    baker_first_name = word(baker_name, 1),
    baker_last_name = word(baker_name, -1)
  ) |> 
  select(-baker_name)

bakes_df =
  read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)

results_df =
  read_csv("data/gbb_datasets/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(
    result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated", 
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

In this section, the datasets were imported, and I reviewed the contents
of each dataset. I noticed that the bakers’ names in the baker.csv file
were full names, whereas the other datasets only contained the bakers’
first names. To ensure consistency across the datasets, I split the full
names in baker.csv into separate columns as baker_first_name and
baker_last_names. After verifying the consistency between the first_name
and last_name columns and the original baker_name colomn, I removed the
baker_name variable to avoid redundant information. I then labeled the
baker as baker_first_name in all datasets, ensuring consistency.
Additionally, in the results.csv dataset, the first two rows contains
non-data values, so they have been removed.The values of result column
in the results.csv were converted into more descriptive phrases, which
is useful for readability and interpretation.

## Section 2: Check for completeness and correctness across datasets.

``` r
anti_join(bakers_df,bakes_df)
```

    ## Joining with `by = join_by(series, baker_first_name)`

    ## # A tibble: 26 × 6
    ##    series baker_age baker_occupation   hometown baker_first_name baker_last_name
    ##     <dbl>     <dbl> <chr>              <chr>    <chr>            <chr>          
    ##  1     10        28 Geography teacher  Essex    Alice            Fevronia       
    ##  2     10        24 Fashion designer   Halifax  Amelia           LeBruin        
    ##  3      9        30 Banker             London   Antony           Amourdoux      
    ##  4      9        33 Full-time parent   Bristol  Briony           Williams       
    ##  5      9        36 Full-time parent   London   Dan              Beasley-Harling
    ##  6     10        32 Support worker     Rotherh… Dan              Chambers       
    ##  7     10        36 International hea… Whitby   David            Atherton       
    ##  8     10        40 Online project ma… Leeds    Helena           Garcia         
    ##  9     10        20 Student            Durham   Henry            Bird           
    ## 10      9        33 Countryside recre… County … Imelda           McCarron       
    ## # ℹ 16 more rows

``` r
anti_join(bakers_df,results_df)
```

    ## Joining with `by = join_by(series, baker_first_name)`

    ## # A tibble: 1 × 6
    ##   series baker_age baker_occupation hometown    baker_first_name baker_last_name
    ##    <dbl>     <dbl> <chr>            <chr>       <chr>            <chr>          
    ## 1      2        41 Housewife        Ongar, Ess… Jo               Wheatley

``` r
anti_join(results_df,bakes_df)
```

    ## Joining with `by = join_by(series, episode, baker_first_name)`

    ## # A tibble: 596 × 5
    ##    series episode baker_first_name technical result
    ##     <dbl>   <dbl> <chr>                <dbl> <chr> 
    ##  1      1       2 Lea                     NA <NA>  
    ##  2      1       2 Mark                    NA <NA>  
    ##  3      1       3 Annetha                 NA <NA>  
    ##  4      1       3 Lea                     NA <NA>  
    ##  5      1       3 Louise                  NA <NA>  
    ##  6      1       3 Mark                    NA <NA>  
    ##  7      1       4 Annetha                 NA <NA>  
    ##  8      1       4 Jonathan                NA <NA>  
    ##  9      1       4 Lea                     NA <NA>  
    ## 10      1       4 Louise                  NA <NA>  
    ## # ℹ 586 more rows

## Section 3: Combining and organizing data into a meaningful structure

``` r
GBbakeoff_df = results_df |> 
  left_join(bakes_df, by = c("series","baker_first_name","episode")) |> 
  left_join(bakers_df, by = c("baker_first_name","series")) |> 
  relocate(series, episode, baker_first_name, baker_last_name, baker_age, baker_occupation, hometown, technical, result,signature_bake, show_stopper)
```

Exporting the final dataset as a CSV:

``` r
write_csv(GBbakeoff_df,"data/GBbakeoff_df.csv")
```

## Section 4: Table creation for Star baker/Winner of each episode in Seasons 5 through 10

``` r
Star_win_df = 
  GBbakeoff_df |> 
  filter(series>=5 & series<=10, result == c("Star Baker","Series Winner")) |> 
  select(series, episode, baker_first_name, baker_last_name, baker_age, result, baker_occupation, hometown, technical,signature_bake, show_stopper)

knitr::kable(
  Star_win_df,format = "html",
  caption = "Table of Star baker/Winner of each episode in Seasons 5 through 10 "
  )
```

<table>
<caption>
Table of Star baker/Winner of each episode in Seasons 5 through 10
</caption>
<thead>
<tr>
<th style="text-align:right;">
series
</th>
<th style="text-align:right;">
episode
</th>
<th style="text-align:left;">
baker_first_name
</th>
<th style="text-align:left;">
baker_last_name
</th>
<th style="text-align:right;">
baker_age
</th>
<th style="text-align:left;">
result
</th>
<th style="text-align:left;">
baker_occupation
</th>
<th style="text-align:left;">
hometown
</th>
<th style="text-align:right;">
technical
</th>
<th style="text-align:left;">
signature_bake
</th>
<th style="text-align:left;">
show_stopper
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Rosemary Seeded Crackers
</td>
<td style="text-align:left;">
Pirates!
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Black Forest Chocolate Fondants
</td>
<td style="text-align:left;">
Tiramisu Baked Alaska
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Kate
</td>
<td style="text-align:left;">
Henry
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Furniture Restorer
</td>
<td style="text-align:left;">
Brighton, East Sussex
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Rhubarb and Custard Tart
</td>
<td style="text-align:left;">
Rhubarb, Prune and Apple Pies
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Chetna
</td>
<td style="text-align:left;">
Makan
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Fashion Designer
</td>
<td style="text-align:left;">
Broadstairs, Kent
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Orange Savarin with Cinnamon Cream
</td>
<td style="text-align:left;">
Almond Liqueur Dobos Torte with Chocolate Caramel Buttercream
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Minted Lamb Pasties
</td>
<td style="text-align:left;">
Stair of Éclairs (Lavender and Blueberry & Rose and Raspberry Éclairs)
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Rose and Pistachio BaklavaWalnut and Almond Baklava
</td>
<td style="text-align:left;">
Hazelnut Mocha EntremetsPink Grapefruit Entremets
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Nancy
</td>
<td style="text-align:left;">
Birtwhistle
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:left;">
Series Winner
</td>
<td style="text-align:left;">
Retired Practice Manager
</td>
<td style="text-align:left;">
Barton-upon-Humber, Lincolnshire
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Apple and Lemon KitesRaspberry and Almond Croissants
</td>
<td style="text-align:left;">
Red Windmill
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Ian
</td>
<td style="text-align:left;">
Cumming
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Travel photographer
</td>
<td style="text-align:left;">
Great Wilbraham, Cambridgeshire
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Orange, Rosemary and Almond Biscotti
</td>
<td style="text-align:left;">
Sandwich de la Confiture
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Ian
</td>
<td style="text-align:left;">
Cumming
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Travel photographer
</td>
<td style="text-align:left;">
Great Wilbraham, Cambridgeshire
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Pomegranate Two Ways Crème Brûlées
</td>
<td style="text-align:left;">
Trio of Spicy and Herby Baked Cheesecakes
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Mat
</td>
<td style="text-align:left;">
Riley
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Fire fighter
</td>
<td style="text-align:left;">
London
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Piña Colada Frangipane Tart
</td>
<td style="text-align:left;">
His ‘n’ Hers Vol-au-vents
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Nadiya
</td>
<td style="text-align:left;">
Hussain
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Full-time mother
</td>
<td style="text-align:left;">
Leeds / Luton
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Rose Pistachio and Mocha Hazelnut Horns
</td>
<td style="text-align:left;">
Bubble Gum and Peppermint Cream Religieuse à l’ancienne
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Candice
</td>
<td style="text-align:left;">
Brown
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
PE teacher
</td>
<td style="text-align:left;">
Barton-Le-Clay, Bedfordshire
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Salted Caramel, Chocolate Iced Shiny Hearts
</td>
<td style="text-align:left;">
Gingerbread Pub with Sticky Ginger Carpet
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Benjamina
</td>
<td style="text-align:left;">
Ebuehi
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Teaching assistant
</td>
<td style="text-align:left;">
South London
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Red Onion Chutney, Brie and Bacon Yorkshire Puddings
</td>
<td style="text-align:left;">
Tropical Churros
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Tom
</td>
<td style="text-align:left;">
Gilliford
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Project engagement manager
</td>
<td style="text-align:left;">
Rochdale
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Blood Orange Halloween Pumpkin Pie
</td>
<td style="text-align:left;">
Floral Tea Cake
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Candice
</td>
<td style="text-align:left;">
Brown
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
PE teacher
</td>
<td style="text-align:left;">
Barton-Le-Clay, Bedfordshire
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Cheesy Cheeky Fish Pies
</td>
<td style="text-align:left;">
Peacock
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Steven
</td>
<td style="text-align:left;">
Carter-Bailey
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Marketer
</td>
<td style="text-align:left;">
Watford, Hertfordshire
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Amarpressi Biscuits
</td>
<td style="text-align:left;">
‘Check Bake’ Game
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Kate
</td>
<td style="text-align:left;">
Lyon
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Health and safety inspector
</td>
<td style="text-align:left;">
Merseyside
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Salted Bay Caramel Millionaire Shortbreads
</td>
<td style="text-align:left;">
Sticky Toffee Apple Caramel Cake
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Liam
</td>
<td style="text-align:left;">
Charles
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Student
</td>
<td style="text-align:left;">
North London
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
‘Standard FC’ Decorative Pies
</td>
<td style="text-align:left;">
‘Nan’s Sunday Dinner’ Pie
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Stacey
</td>
<td style="text-align:left;">
Hart
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Former school teacher
</td>
<td style="text-align:left;">
Radlett, Hertfordshire
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Camembert & Onion and Apple & Blueberry Bedfordshire Clangers
</td>
<td style="text-align:left;">
‘Bright’ Lemon & Orange Savoy Cake
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Rahul
</td>
<td style="text-align:left;">
Mandal
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Research scientist
</td>
<td style="text-align:left;">
Rotherham
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Dan
</td>
<td style="text-align:left;">
Beasley-Harling
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Full-time parent
</td>
<td style="text-align:left;">
London
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Kim-Joy
</td>
<td style="text-align:left;">
Hewlett
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Mental health specialist
</td>
<td style="text-align:left;">
Leeds
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Briony
</td>
<td style="text-align:left;">
Williams
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Full-time parent
</td>
<td style="text-align:left;">
Bristol
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Ruby
</td>
<td style="text-align:left;">
Bhogal
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Project manager
</td>
<td style="text-align:left;">
London
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Michelle
</td>
<td style="text-align:left;">
Evans-Fecci
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Print shop administrator
</td>
<td style="text-align:left;">
Tenby, Wales
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Alice
</td>
<td style="text-align:left;">
Fevronia
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Geography teacher
</td>
<td style="text-align:left;">
Essex
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Michael
</td>
<td style="text-align:left;">
Chakraverty
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Theatre manager/fitness instructor
</td>
<td style="text-align:left;">
Stratford-upon-Avon
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Steph
</td>
<td style="text-align:left;">
Blackwell
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Shop assistant
</td>
<td style="text-align:left;">
Chester
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Steph
</td>
<td style="text-align:left;">
Blackwell
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
Star Baker
</td>
<td style="text-align:left;">
Shop assistant
</td>
<td style="text-align:left;">
Chester
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
David
</td>
<td style="text-align:left;">
Atherton
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:left;">
Series Winner
</td>
<td style="text-align:left;">
International health adviser
</td>
<td style="text-align:left;">
Whitby
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
NA
</td>
<td style="text-align:left;">
NA
</td>
</tr>
</tbody>
</table>

In this section, the question asks about the star baker/winner of each
episode in Seasons 5 through 10. I filtered out these persons and formed
a reader-friendly table. The key information, such as the which series
and episode the star baker/winner attend and results for illustrating
they were either star baker or winner, were placed in front. Other
personally relevant information of each participant was placed
afterwards.

According to the table, the first thing I noticed is that Richard Burr
earned “Star Baker” title in four episodes of Series 5. However, Nancy
Birtwhistle was the “Series Winner” for that series. Similarly, in
Series 6, Ian Cumming won “Star Backer” in two episodes, and Candice
Brown did the same in Series 7. In Series 9, there were five different
“Star Bakers” which is one more than in the other series. In Series 10,
Steph Balckwell earned two “Star baker” title in two episodes, yet David
Atherton was the “Series winner”. Interestingly, there is no “Series
Winner” recored for Series 6, 7, 8, and 9.In conclusion, based on the
patterns observed in this table, it is clear that there were no any
predictable overall winners. What stands out the most is that
multiple_time “Star Bakers” do not always end up with winning the
series, which goes againt what many people might expect. This makes the
outcome of each series more surprising and unpredictable.

## Section 5: Dataset exploration of viewers.csv

``` r
viewers_df =
  read_csv("data/gbb_datasets/viewers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names()
```

The first 10 rows of this dataset:

``` r
head(viewers_df,10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
Ave_viewership1 = mean(pull(viewers_df,series_1), na.rm = TRUE)
  
Ave_viewership5 = mean(pull(viewers_df,series_5))
```

By checking the original imported dataset, I found there are missing
values (NAs) for series_1 whereas series_5 contains no missing value.

The average viewership for Season 1 was 2.77, and for Season 5 was
10.0393.
